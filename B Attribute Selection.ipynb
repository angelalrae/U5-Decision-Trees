{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 310](https://github.com/GonzagaCPSC310) Data Mining\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Attribute Selection\n",
    "What are our learning objectives for this lesson?\n",
    "* Use entropy to select attributes to split on\n",
    "* Create decision trees with continuous attributes\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Attributes\n",
    "Resulting decision tree depends on attribute selection approach\n",
    "* Want high predictive accuracy with small number of rules\n",
    "* In practice, using **\"information gain\"** does well (popular approach)\n",
    "\n",
    "Basic idea:\n",
    "* Select attribute with the largest \"information gain\"\n",
    "* Typically the attribute that most \"unevenly\" partitions the instances on class\n",
    "\n",
    "How it works: use \"Shannon Entropy\" as a measure of information content\n",
    "* Q: how many bits are needed to represent numbers between 1 and 64?\n",
    "    * $log_2 64 = 6$ bits\n",
    "* What if instead we had messages involving combinations of 64 words\n",
    "    * Could capture each word using a 6 bit number\n",
    "    * Thus a message with 10 words would cost 10 × 6 = 60 bits\n",
    "* However, if we knew more about the distribution of words, we could (on average) use fewer bits per message!\n",
    "    * e.g., \"the\" occurs more than any other word\n",
    "    * Use encodings whose lengths are inversely proportional to their frequencies (probabilities)\n",
    "* Entropy gives a precise lower bound for expected number of bits needed to encode a message (based on a probability distribution)\n",
    "\n",
    "## Entropy $E$\n",
    "The details:\n",
    "* Entropy $E = 0$ implies low content (e.g., all values are the same)\n",
    "* Highest entropy value when all values equally likely (most content)\n",
    "* Entropy formula assumes information encoded in bits ... (thus, $log_2$)\n",
    "$$E = -\\sum_{i=1}^{n}p_i log_2(p_i)$$\n",
    "    * $n$ for us is the number of class labels\n",
    "    * $p_i$ is proportion of instances labeled with i (i.e., $P(C)$ for $C = i$)\n",
    "    * Note $p_i$ is assumed to be strictly greater than 0, up to and including 1\n",
    "* What the formula is saying:\n",
    "    * Since $0 < p_i \\leq 1$, we know that $-p_i log_2(p_i) \\geq 0$ is positive\n",
    "    * e.g., for $log_2(0.5) = y$, we have $2^y = \\frac{1}{2}$, which means $y = -1$\n",
    "    * If $p_i = 1$, then $-p_i log_2(p_i) = 0$\n",
    "    * $E$ has the highest value when labels are equally distributed\n",
    "    \n",
    "The function $-p_i log_2(p_i)$ for $0 < p_i \\leq 1$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC310/U5-Decision-Trees/master/figures/entropy_graph.png\" width=\"500\"/>\n",
    "\n",
    "The function $-(p_i log_2(p_i) + (1-p_i) log_2 (1 - p_i))$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC310/U5-Decision-Trees/master/figures/entropy_binary_classification_graph.png\" width=\"500\"/>\n",
    "\n",
    "* This mimics having just two labels\n",
    "* As shown, the closer the two labels the higher the entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the attribute that maximizes information gain\n",
    "* Information Gain = $E_{start} - E_{new}$\n",
    "    * At each partition, pick attribute with highest information gain\n",
    "    * That is, split on attribute with greatest reduction in entropy\n",
    "    * Which means find attribute with smallest $E_{new}$\n",
    "\n",
    "### Example Computing $E$\n",
    "Assume we have: $p_{yes} = 3/8$ and $p_{no} = 5/8$\n",
    "\n",
    "Q: What is $E$ for this distribution? ... in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9544340029249649\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "p_yes = 3 / 8\n",
    "p_no = 5 / 8\n",
    "E = -(p_yes * math.log(p_yes, 2) + p_no * math.log(p_no, 2))\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now assume we have: $p_{yes} = 2/8$ and $p_{no} = 6/8$\n",
    "\n",
    "Q: Will $E$ for this distribution be larger or smaller? ... should be smaller!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "p_yes = 2 / 8\n",
    "p_no = 6 / 8\n",
    "E = -(p_yes * math.log(p_yes, 2) + p_no * math.log(p_no, 2))\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating $E_{new}$ for an Attribute\n",
    "Assume we want to partition $D$ on an attribute $A$\n",
    "* Where $A$ has values $a_1, a_2, ... , a_v$\n",
    "* Creating partitions $D_1, D_2, ... , D_v$\n",
    "\n",
    "We'd like each partition to be \"pure\" (contain instances of same class label)\n",
    "* They may not be, however (i.e., they may have \"clashes\")\n",
    "* The amount of additional info still needed for a \"pure\" classification is:\n",
    "$$E_{new} = \\sum_{j=1}^{v} \\frac{|D_j|}{|D|} \\times E_{D_j}$$\n",
    "\n",
    "* where $E_{D_j}$ is the entropy of partition $D_j$\n",
    "* $\\frac{|D_j|}{|D|}$ is the likelihood an instance is in the $j$-th partition\n",
    "* Thus, $E_{new}$ is a **weighted average** of the attributes corresponding partitions\n",
    "\n",
    "We pick the attribute with the smallest $E_{new}$ value\n",
    "* This means the smallest amount of information needed to classify an instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Tasks\n",
    "\n",
    "|standing |job_status |credit_rating |buys_iphone|\n",
    "|-|-|-|-|\n",
    "|1 |3 |fair |no|\n",
    "|1 |3 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|2 |1 |fair |yes|\n",
    "|2 |1 |excellent |no|\n",
    "|2 |1 |excellent |yes|\n",
    "|1 |2 |fair |no|\n",
    "|1 |1 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|1 |2 |excellent |yes|\n",
    "|2 |2 |excellent |yes|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "\n",
    "1. Calculate $E_{new}$ for job_status for whole table\n",
    "1. Calculate $E_{new}$ for credit_rating for whole table\n",
    "1. What is the general $E_{new}$ algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Continuous Attributes\n",
    "* So far we've mainly assumed categorical attributes\n",
    "* Like always we can discretize continuous attributes first\n",
    "* Alternatively, we can use Entropy to find a \"split point\"\n",
    "\n",
    "The random approach:\n",
    "* Pick a random value $v$ from the set of values in the attribute\n",
    "* Use $v$ as a \"split point\"\n",
    "* i.e., divide into two partitions: $\\leq v$ and $> v$\n",
    "\n",
    "Using entropy instead:\n",
    "1. Sort the values in ascending order $[v_1, v_2, ... , v_k]$\n",
    "1. For each split point $v$ in $v_1$ through $v_{k−1}$ calculate $E_{new}$\n",
    "    * Thus, two partitions in each case ($\\leq v$ and $> v$)\n",
    "    * Alternatively, use half-way points between adjacent values $v_i$ and $v_{i+1}$\n",
    "1. Select the split point that minimizes $E_{new}$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
