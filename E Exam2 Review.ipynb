{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 310](https://github.com/GonzagaCPSC310) Data Mining\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Exam #2 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Task(s)\n",
    "1. Log in to the Windows lab machine\n",
    "1. Open a command prompt\n",
    "1. Launch the Python interpreter\n",
    "1. `import math`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Bring?\n",
    "* Two sharp pencils and a good eraser\n",
    "* Data mining and Python programming knowledge in your head :)\n",
    "\n",
    "## What NOT to Bring?\n",
    "* Electronics (including calculators) may not be used during the exam!\n",
    "    * EXCEPT the math module in the Python interpreter as a calculator ONLY\n",
    "* Notes may not be used during the exam!\n",
    "    * You may use your RQ note cards if you turned them in\n",
    "\n",
    "Note: If you are caught cheating, your exam will be confiscated and you will receive a 0.\n",
    "\n",
    "Note: Use the restroom before class. Once testing starts, the only reason for leaving the classroom is turning in your exam as done.\n",
    "\n",
    "## Exam Format\n",
    "The exam will be closed-book, closed-notes (except for RQ notecards), and closed-friends. The exam format is paper pencil and it will have a mix of concept questions and programming questions.\n",
    "\n",
    "## Exam Time Frame\n",
    "Please be aware that, because you will be taking the exam during a normal lecture period (50 minutes), time will be extremely tight for the exam so work efficiently and manage your time well. If you show up late to class, you will have less time to take the exam.\n",
    "\n",
    "## Exam Coverage\n",
    "The exam covers everything we have covered so far in the course. Here is a general outline of the topics we have covered so far (note that this list is not comprehensive of everything we have covered so far, but a general outline):\n",
    "\n",
    "### U0-3\n",
    "See the Exam #1 Review Guide.\n",
    "\n",
    "\n",
    "### U4 Supervised Learning (RQ4, 5-Bramer Ch. 3, 7)\n",
    "1. What is machine learning and why do we use it?\n",
    "    1. Labeled vs. unlabeled data\n",
    "    1. Supervised vs. unsupervised learning\n",
    "    1. What is a class?\n",
    "    1. Classification vs. regressions\n",
    "    1. Example algorithms of each type\n",
    "    1. Training vs. testing\n",
    "1. What is nearest neighbor classification?\n",
    "    1. What is kNN and why do we use it?\n",
    "    1. What type of data (attributes and classes) is for use with kNN? \n",
    "    1. What are good values for k? How do you determine this?\n",
    "    1. How to pick the class? What happens if there are ties?\n",
    "    1. Know the basic kNN algorithm\n",
    "1. What are distance functions and why do we use them?\n",
    "    1. Properties of distance functions\n",
    "    1. Examples of distance functions\n",
    "    1. What is the most commonly used distance function?\n",
    "        1. Euclidean\n",
    "        1. What is the formula and how do you use it with kNN?\n",
    "1. What is normalization and why do we use it?\n",
    "    1. What is the formula and how do you use it with kNN?\n",
    "    1. How to normalize categorical values?\n",
    "    1. What to do with missing values?\n",
    "1. Evaluating classifier performance\n",
    "    1. What are TP, FP, TN, and FN?\n",
    "    1. Using confusion matrices\n",
    "    1. Using common classification metrics\n",
    "        1. Accuracy and error rate (binary and multi-class)\n",
    "        1. Precision, recall, F-measure (binary and multi-class)\n",
    "1. Generating train/test sets\n",
    "    1. What is overfitting and why do we want to avoid it?\n",
    "    1. What are the different approaches and advantages/disadvantages of each?\n",
    "        1. The holdout method\n",
    "        1. Random subsampling\n",
    "        1. k-Fold cross validation (and stratified!)\n",
    "        1. Bootstrap method\n",
    "1. Know basic probability\n",
    "    1. What is a prior?\n",
    "    1. What is a posterior?\n",
    "1. What is Bayes' Theorem and why is it relevant for classification?\n",
    "1. What is Naive Bayes' classification and why do we use it?\n",
    "    1. What is the conditional independence assumption and why is it relevant?\n",
    "    1. What type of data (attributes and classes) is for use with Naive Bayes? \n",
    "    1. How to pick the class?\n",
    "    1. Know the basic Naive Bayes' algorithm\n",
    "    1. How do we handle continuous attributes with Naive Bayes?\n",
    "        1. What is a normal (AKA Gaussian) distribution?\n",
    "        1. How do we use a Gaussian distribution to compute posterior probabilities?\n",
    "\n",
    "### U5 Decision Trees (RQ6, 7-Bramer Ch. 4, 5, 9)\n",
    "1. kNN and Naive Bayes are \"instance-at-a-time\" classifiers\n",
    "1. Decision trees are \"rule-based\" classifiers\n",
    "1. What are rules and how do we use them?\n",
    "1. How are rules represented in decision trees?\n",
    "1. How do we make a classification using a decision tree?\n",
    "1. What is the adequacy condition and why is it important?\n",
    "1. What is a clash?\n",
    "1. Know the basic Top-Down Induction of Decision Trees (TDIDT) algorithm\n",
    "    1. How do we select an attribute? What are different approaches?\n",
    "    1. How do we create partitions?\n",
    "    1. What are the different recursive base cases?\n",
    "1. What are different ways to resolve clashes? What are advantages/disadvantages of each approach?\n",
    "1. What are different ways to handle case 3 (no more instances to partition)? What are advantages/disadvantages?\n",
    "1. What is entropy and why do we use it for attribute selection?\n",
    "    1. What is the formula and how do you use it?\n",
    "    1. What is information gain and how do you compute it?\n",
    "1. How do we handle continuous attributes with decision trees?\n",
    "1. What are different ways to represent a decision tree in Python?\n",
    "1. What is pruning and why do we use it?\n",
    "    1. Generalizing vs. specializing a rule\n",
    "    1. Pre vs. post pruning and their different approaches\n",
    "    1. How do you pre-prune a tree?\n",
    "        1. What are termination conditions for pre-pruning?\n",
    "    1. How do you post-prune a tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Strategy for Preparing for the Exam\n",
    "The exam questions will require you to solve problems in a variety of formats: defining key terms/concepts, applying algorithms to example datasets, writing code, determining the output of code, etc.\n",
    "\n",
    "I recommend that you use the following activities and materials to prepare for the exam:\n",
    "* Review reading quizzes, warm-up tasks, programming assignments, problems solved in class, and code derived in class\n",
    "    * These may well be your best resources\n",
    "    * An excellent learning activity would be to re-solve these exercises\n",
    "* Lesson notes\n",
    "* Re-read the pages assigned for the reading quizzes\n",
    "    * Solve the problems included at the end of the chapters\n",
    "\n",
    "## Extra Practice Problems\n",
    "### 1\n",
    "1. What is normalization and why do we use it?\n",
    "1. Normalize the following list of values: \\[3, 6, 5, 8, 7, 6, 11, 2, 13, 4, 6\\]\n",
    "\n",
    "### 2\n",
    "Using the (fake) iPhone Purchases data from class:\n",
    "1. Create two different decision trees for the fake iPhone dataset. Use different approaches for attribute selection such as random selection or take first.\n",
    "1. Provide the total number of matching instances for each leaf node. \n",
    "1. Which decision tree do you think is \"better\"? Why?\n",
    "\n",
    "### 3\n",
    "Using the (fake) iPhone Purchases data from class:\n",
    "1. Calculate $E_{new}$ for the `job_status` attribute using the entire table as the current partition.\n",
    "1. Calculate $E_{new}$ for the `credit_rating` attribute using the entire table as the current partition.\n",
    "\n",
    "### 4\n",
    "Consider the following labeled dataset, where result denotes class information and the remaining columns have categorical values.\n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|1|5|yes|\n",
    "|2|6|yes|\n",
    "|1|5|no|\n",
    "|1|5|no|\n",
    "|1|6|yes|\n",
    "|2|6|no|\n",
    "|1|5|yes|\n",
    "|1|6|yes|\n",
    "|2|5|no|\n",
    "|2|6|yes|\n",
    "\n",
    "1. Calculate $E_{new}$ for `att1` assuming the table is the starting partition.\n",
    "1. Calculate $E_{new}$ for `att2` assuming the table is the starting partition.\n",
    "1. Based on your answers for questions 1 and 2, which attribute should be selected using the information gain approach for attribute selection? Explain your answer.\n",
    "\n",
    "### 5\n",
    "1. Write a function `def attribute_frequencies(instances, att_index, class_index)` that returns the class frequencies for each attribute value in the form: `{att_val:[{class1: freq, class2: freq, ...}, total], ...}`\n",
    "1. Write a function `calc_enew(instances, att_index, class_index)` that calculates $E_{new}$ for a partition of `instances` by `att_index`. Use your `attribute_frequencies(instances, att_index, class_index)` function from the previous question.\n",
    "\n",
    "### 6\n",
    "Write the pseudocode algorithm for TDIDT and provide different options for handling the cases of no more attributes to use for partitioning and no more instances to partition.\n",
    "\n",
    "### 7\n",
    "Consider the following dataset:\n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|3|2|no|\n",
    "|6|6|yes|\n",
    "|4|1|no|\n",
    "|4|4|no|\n",
    "|1|2|yes|\n",
    "|2|0|no|\n",
    "|0|3|yes|\n",
    "|1|6|yes|\n",
    "\n",
    "1. Assume we want to perform Stratified k-Fold Cross-Validation of our NN classifier for $k$ = 4. Create corresponding folds (partitions) for the dataset.\n",
    "2. Describe how these $k$ folds would be used to perform cross validation. That is, show how the $k$ test runs are performed.\n",
    " \n",
    "### 8\n",
    "Suppose we have built a tree to classify whether a passenger aboard the [Titanic](https://en.wikipedia.org/wiki/RMS_Titanic) survived the shipwreck or not:\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png\" width=\"400\"/>\n",
    "\n",
    "(image from [https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png))\n",
    "\n",
    "Where \"sex\" is the gender of the passenger, \"age\" is the age of the passenger in years (fractional if age is less than one), and \"sibsp\" is the number of siblings/spouses aboard the vessel. \n",
    "\n",
    "For the following test set, use the tree to predict \"survived\" or \"died\" for each instance:\n",
    "\n",
    "|sex|age|sibsp|prediction|\n",
    "|-|-|-|-|\n",
    "|female|30|0|?|\n",
    "|male|45|2|?|\n",
    "|male|8|0|?|\n",
    "|male|6|3|?|\n",
    "\n",
    "### 9\n",
    "1. Re-work RQ5 (take home Naive Bayes classification)\n",
    "1. Re-work Intro to Decision Trees lab and Entropy lab.\n",
    "1. Re-work all of the warm-up and lab tasks interleaved throughout the U4 and U5 notes.\n",
    "\n",
    "### 10\n",
    "Write Python code to make the following graph:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC310/U5-Decision-Trees/master/figures/entropy_graph.png\" width=\"400\"/>\n",
    "\n",
    "### 11\n",
    "Consider the following labeled dataset, where result denotes class information and the remaining columns have categorical values.\n",
    "\n",
    "|att1|att2|result|\n",
    "|-|-|-|\n",
    "|1|5|yes|\n",
    "|2|6|yes|\n",
    "|1|5|no|\n",
    "|1|5|no|\n",
    "|1|6|yes|\n",
    "|2|6|no|\n",
    "|1|5|yes|\n",
    "|1|6|yes|\n",
    "|2|5|no|\n",
    "|2|6|yes|\n",
    "|1|5|yes|\n",
    "\n",
    "1. Draw the decision tree for this data set that first partitions on `att1` and then on `att2`. Give the associated number of yes and no values in each leaf node.\n",
    "1. What are the clashes in the decision tree?\n",
    "1. Write the rules implied by the decision tree, assuming majority voting is used to resolve clashes.\n",
    "\n",
    "<img src=\"http://www.quickmeme.com/img/53/53e6adbb95c7ddfdc8efbec85fbf24097b3a873f17e5026ba6e520e803d7040b.jpg\" width=\"300\">\n",
    "\n",
    "### 12\n",
    "This problem is adapted from [this site](https://www.listendata.com/2017/12/k-nearest-neighbor-step-by-step-tutorial.html).\n",
    "\n",
    "Suppose we have height, weight and T-shirt size of some customers and we need to predict the T-shirt size of a new customer given only height and weight information we have. Data including height, weight and T-shirt size information is shown below:\n",
    "\n",
    "|Height (in cms)\t|Weight (in kgs)\t|T Shirt Size|\n",
    "|-|-|-|\n",
    "|158\t|58\t|M|\n",
    "|158\t|59\t|M|\n",
    "|158\t|63\t|M|\n",
    "|160\t|59\t|M|\n",
    "|160\t|60\t|M|\n",
    "|163\t|60\t|M|\n",
    "|163\t|61\t|M|\n",
    "|160\t|64\t|L|\n",
    "|163\t|64\t|L|\n",
    "|165\t|61\t|L|\n",
    "|165\t|62\t|L|\n",
    "|165\t|65\t|L|\n",
    "|168\t|62\t|L|\n",
    "|168\t|63\t|L|\n",
    "|168\t|66\t|L|\n",
    "|170\t|63\t|L|\n",
    "|170\t|64\t|L|\n",
    "|170\t|68\t|L|\n",
    "\n",
    "Let's say there is a new customer named 'Monica' has height 161cm and weight 61kg. What is your prediction of her T-shirt size? Don't forget to normalize the dataset before applying $k$NN!!\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*AuXDgGrr0wbCoF6KDXXSZQ.jpeg\" width=\"200\">\n",
    "\n",
    "### 13\n",
    "This problem is adapted from [this site](http://www.inf.u-szeged.hu/~ormandi/ai2/06-naiveBayes-example.pdf)\n",
    "\n",
    "Suppose we have the following dataset with attributes `Color`, `Type`, `Origin`, and the class, `Stolen`, can be either yes or no. Using Naive Bayes, predict whether a Red Domestic SUV will be stolen or not.\n",
    "\n",
    "|Example No. |Color |Type |Origin |Stolen?|\n",
    "|-|-|-|-|-|\n",
    "|1 |Red |Sports |Domestic |Yes|\n",
    "|2 |Red |Sports |Domestic |No|\n",
    "|3 |Red |Sports |Domestic |Yes|\n",
    "|4 |Yellow |Sports |Domestic |No|\n",
    "|5 |Yellow |Sports |Imported |Yes|\n",
    "|6 |Yellow |SUV |Imported |No|\n",
    "|7 |Yellow |SUV |Imported |Yes|\n",
    "|8 |Yellow |SUV |Domestic |No|\n",
    "|9 |Red |SUV |Imported |No|\n",
    "|10 |Red |Sports |Imported |Yes|\n",
    "\n",
    "<img src=\"https://memegenerator.net/img/instances/63950930/implement-a-naive-bayes-classifier-aint-nobody-got-tiime-for-that.jpg\" width=\"300\">\n",
    "\n",
    "Nice work on making it through the practice problems!! Do well on the exam!!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
