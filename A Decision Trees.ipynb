{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CPSC 310](https://github.com/GonzagaCPSC310) Data Mining\n",
    "[Gonzaga University](https://www.gonzaga.edu/)\n",
    "\n",
    "[Gina Sprint](http://cs.gonzaga.edu/faculty/sprint/)\n",
    "\n",
    "# Decision Trees\n",
    "What are our learning objectives for this lesson?\n",
    "* Learn decision tree terminology\n",
    "* Introduce the TDIDT algorithm and clashes\n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Dr. Shawn Bowers' Data Mining notes\n",
    "* [Data Science from Scratch](https://www.amazon.com/Data-Science-Scratch-Principles-Python/dp/149190142X/ref=sr_1_1?ie=UTF8&qid=1491521130&sr=8-1&keywords=joel+grus) by Joel Grus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Decision Tree Classifiers\n",
    "$k$-NN and Naive Bayes are \"instance-at-a-time\" classifiers\n",
    "* Given a new instance, use training set to predict class label\n",
    "* Hard to know \"why\" or what overall pattern led to prediction\n",
    "* Highly dependent on particular instance given (its attribute values)\n",
    "\n",
    "Decision trees are \"rule\"-based classifiers\n",
    "* Build a set of general rules from training set\n",
    "* Like a \"compiled\" version of the training set\n",
    "* Use rules (not training set) to classify new instances\n",
    "\n",
    "Rules are basic if-then statements:\n",
    "\n",
    ">IF $att_1 = val_1^1 \\wedge att_2 = val_1^2 \\wedge ...$ THEN $class = label_1$\n",
    "\n",
    ">IF $att_1 = val_2^1 \\wedge att_2 = val_2^2 \\wedge ...$ THEN $class = label_2$\n",
    "\n",
    ">IF $att_1 = val_3^1 \\wedge att_2 = val_3^2 \\wedge ...$ THEN $class = label_3$\n",
    "\n",
    "The rules are captured in a \"decision tree\"\n",
    "* Internal nodes denote attributes (e.g., job status, standing, etc.)\n",
    "* Edges denote values of the attribute\n",
    "* Leaves denote class labels (e.g., buys iphone = yes)\n",
    "    * Either stating a prediction\n",
    "    * Or giving the distribution...\n",
    "\n",
    "\n",
    "### Lab Task 1\n",
    "An example for the iphone prediction example. iPhone Purchases (Fake) dataset:\n",
    "\n",
    "|standing |job_status |credit_rating |buys_iphone|\n",
    "|-|-|-|-|\n",
    "|1 |3 |fair |no|\n",
    "|1 |3 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|2 |1 |fair |yes|\n",
    "|2 |1 |excellent |no|\n",
    "|2 |1 |excellent |yes|\n",
    "|1 |2 |fair |no|\n",
    "|1 |1 |fair |yes|\n",
    "|2 |2 |fair |yes|\n",
    "|1 |2 |excellent |yes|\n",
    "|2 |2 |excellent |yes|\n",
    "|2 |3 |fair |yes|\n",
    "|2 |2 |excellent |no|\n",
    "|2 |3 |fair |yes|\n",
    "\n",
    "A *clash* is when two or more instances in a partition have the same combination of attribute values but different classifications. \n",
    "\n",
    "Bramer's definition of the Top-Down Induction of Decision Trees (TDIDT) assumes the *adequacy condition*, which ensures that no two instances with identical attribute values have different class labels (e.g. no clashes).\n",
    "\n",
    "Does the iPhone dataset have any clashes?\n",
    "\n",
    "### Lab Task 2\n",
    "Here is an example decision tree for the iPhone dataset:\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC310/U5-Decision-Trees/master/figures/iphone_decision_tree_example.png\" width=\"850\"/>\n",
    "\n",
    "* Note that attribute values as inner \"oval\" nodes (not edge labels)\n",
    "* These represent \"partitions\" of the training set\n",
    "* Leaf nodes give distribution of class labels\n",
    "\n",
    "Extract the rules from this decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TDIDT (Top-Down Induction of Decision Trees) Algorithm\n",
    "Basic Approach (uses recursion!):\n",
    "* At each step, pick an attribute (\"attribute selection\")\n",
    "* Partition data by attribute values ... this creates pairwise disjoint partitions\n",
    "* Repeat until one of the following occurs (base cases):\n",
    "    1. Partition has only class labels that are the same ... no clashes, make a leaf node\n",
    "    2. No more attributes to partition ... reached the end of a branch and there may be clashes, see options below\n",
    "    3. No more instances to partition ... see options below\n",
    "\n",
    "    \n",
    "### More on Case 3\n",
    "Assume we have the following:\n",
    "<img src=\"https://raw.githubusercontent.com/GonzagaCPSC310/U5-Decision-Trees/master/figures/decision_tree_one_attr.png\" width=\"300\"/>\n",
    "\n",
    "* Where the partition for att1=v1 has many instances\n",
    "* But the partition for att1=v2 has no instances\n",
    "* What are our options?\n",
    "    1. Do Nothing: Leave value out of tree (creates incomplete decision tree)\n",
    "    2. Backtrack: replace Attribute 1 node with leaf node (possibly w/clashes, see options below)\n",
    "* For the first choice, we won't be able to classify all instances\n",
    "* We also need to know the possible attribute values ahead of time\n",
    "\n",
    "### Handling Clashes for Prediction\n",
    "1. \"Majority Voting\"... select the class with highest number of instances\n",
    "2. \"Intuition\"... that is, use common sense and pick one (hand modify tree)\n",
    "3. \"Discard\"... remove the branch from the node above\n",
    "    * Similar to case 3 above\n",
    "    * Results in \"missing\" attribute combos (some instances can't be classified)\n",
    "    * e.g., just remove two 50/50 branches from iPhone example tree\n",
    "\n",
    "### Lab Task 3\n",
    "Use TDIDT to create a decision tree for iPhone example\n",
    "* Randomly select attributes as your \"attribute selection\" approach\n",
    "* Extract the rules for your decision tree\n",
    "\n",
    "### Lab Task 4\n",
    "Consider the following data set. In this dataset, each instance example is an attribute list describing a job candidate:\n",
    "* Level of expertise (string): Junior, Mid, Senior\n",
    "* Preferred language (string): Java, Python, R\n",
    "* Whether she is active on twitter (boolean): yes, no\n",
    "* Whether she has a PhD (boolean): yes, no\n",
    "* CLASS: Interviewed well? (boolean): True, False\n",
    "    \n",
    "|level|lang|tweets|phd|interviewed_well|\n",
    "|-|-|-|-|-|\n",
    "|Senior|Java|no|no|False|\n",
    "|Senior|Java|no|yes|False|\n",
    "|Mid|Python|no|no|True|\n",
    "|Junior|Python|no|no|True|\n",
    "|Junior|R|yes|no|True|\n",
    "|Junior|R|yes|yes|False|\n",
    "|Mid|R|yes|yes|True|\n",
    "|Senior|Python|no|no|False|\n",
    "|Senior|R|yes|no|True|\n",
    "|Junior|Python|yes|no|True|\n",
    "|Senior|Python|yes|yes|True|\n",
    "|Mid|Python|no|yes|True|\n",
    "|Mid|Java|yes|no|True|\n",
    "|Junior|Python|no|yes|False|\n",
    "\n",
    "Construct a decision tree that first splits on `level`. For the `Senior` partition, split on `tweets`. For the `Junior` partition, split on `phd`. Label each of your leaf nodes with the class proportion of the partition of that subtree.\n",
    "\n",
    "### Lab Task 5\n",
    "Use your tree from the previous task to classify the following test instances:\n",
    "1. $X_{1}$ = `[\"Junior\", \"Java\", \"yes\", \"no\"]`\n",
    "1. $X_{2}$ = `[\"Junior\", \"Java\", \"yes\", \"yes\"]`"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
